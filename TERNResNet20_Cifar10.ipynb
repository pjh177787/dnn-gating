{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.utils as util\n",
    "\n",
    "import numpy as np\n",
    "import os, time, sys\n",
    "import argparse\n",
    "\n",
    "import utils.pg_utils as q\n",
    "import model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform_train = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomCrop(32, 4),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "        ])\n",
    "\n",
    "    # pin_memory=True makes transfering data from host to GPU faster\n",
    "    trainset = torchvision.datasets.CIFAR10(root='/tmp/cifar10_data', train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='/tmp/cifar10_data', train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    dataset_sizes = [len(trainset), len(testset)]\n",
    "    \n",
    "    return trainloader, testloader, classes, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.tern_resnet_cifar as m\n",
    "def generate_model():\n",
    "    return m.tern_resnet20(10)\n",
    "#     if model_arch == 'resnet-20':\n",
    "#         if pg:\n",
    "#             import model.pg_cifar10_resnet_s as m\n",
    "#             kwargs = {'wbits':wbits, 'abits':abits, \\\n",
    "#                       'pred_bits':pbits, 'sparse_bp':sparse_bp, \\\n",
    "#                       'pact':pact}\n",
    "#             return m.resnet20(**kwargs)\n",
    "#         else:\n",
    "#             import model.quantized_cifar10_resnet as m\n",
    "#             kwargs = {'wbits':wbits, 'abits':abits, 'pact':pact}\n",
    "#             return m.resnet20(**kwargs)\n",
    "#     else:\n",
    "#         raise NotImplementedError(\"Model architecture is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "import copy\n",
    "\n",
    "def train_model(trainloader, dataset_sizes, testloader, net, device):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        print(\"Activate multi GPU support.\")\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    # define the loss function\n",
    "    criterion = (nn.CrossEntropyLoss().cuda() \n",
    "                if torch.cuda.is_available() else nn.CrossEntropyLoss())\n",
    "    # Scale the lr linearly with the batch size. \n",
    "    # Should be 0.1 when batch_size=128\n",
    "    initial_lr = 0.0001\n",
    "    # initialize the optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), \n",
    "                          lr=initial_lr, \n",
    "                          momentum=0.9,\n",
    "                          weight_decay=1e-4)\n",
    "    # multiply the lr by 0.1 at 100, 150, and 200 epochs\n",
    "    div = num_epoch // 4\n",
    "    lr_decay_milestones = [div*2, div*3]\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                        optimizer, \n",
    "                        milestones=lr_decay_milestones, \n",
    "                        gamma=0.1,\n",
    "                        last_epoch=-1)\n",
    "    \n",
    "    # some bookkeeping\n",
    "    since = perf_counter()\n",
    "    liveloss = PlotLosses()\n",
    "    best_acc = 0.0\n",
    "    best = 0\n",
    "    \n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    max_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epoch): # loop over the dataset multiple times\n",
    "        ts = perf_counter()\n",
    "        # set printing functions\n",
    "        batch_time = util.AverageMeter('Time/batch', ':.3f')\n",
    "        losses = util.AverageMeter('Loss', ':6.2f')\n",
    "        top1 = util.AverageMeter('Acc', ':6.2f')\n",
    "#         progress = util.ProgressMeter(\n",
    "#                         len(trainloader),\n",
    "#                         [losses, top1, batch_time],\n",
    "#                         prefix=\"Epoch: [{}]\".format(epoch+1)\n",
    "#                         )\n",
    "\n",
    "        # switch the model to the training mode\n",
    "        net.train()\n",
    "\n",
    "        print('current learning rate = {}'.format(optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        # each epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        end = perf_counter()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            for name, param in net.named_parameters():\n",
    "                if 'threshold' in name:\n",
    "                    loss += sigma * torch.norm(param-gtarget)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            _, batch_predicted = torch.max(outputs.data, 1)\n",
    "            batch_accu = 100.0 * (batch_predicted == labels).sum().item() / labels.size(0)\n",
    "            batch_loss = loss.item() * inputs.size(0)\n",
    "            losses.update(loss.item(), labels.size(0))\n",
    "            top1.update(batch_accu, labels.size(0))\n",
    "            running_loss += batch_loss\n",
    "            running_corrects += torch.sum(batch_predicted == labels.data)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_elapsed = perf_counter() - end\n",
    "            batch_time.update(batch_elapsed)\n",
    "            end = perf_counter()\n",
    "\n",
    "#             progress.display(i) # i = batch id in the epoch\n",
    "            \n",
    "            print(\"\\r[Epoch:%d Iteration:%d/%d] Loss: %.3f, Accuracy: %.3f, Time: %.3f\"\n",
    "                  %(epoch+1, i+1, len(trainloader), batch_loss, batch_accu, batch_elapsed), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_sizes[0]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[0]\n",
    "        loss_list.append(epoch_loss)\n",
    "        accuracy_list.append(epoch_acc)\n",
    "        \n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # print test accuracy every epochs\n",
    "#         print('epoch {}'.format(epoch+1))\n",
    "        val_loss, val_acc = test_accu(testloader, net, device)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "\n",
    "        liveloss.update({\n",
    "                'log loss': epoch_loss,\n",
    "                'val_log loss': val_loss,\n",
    "                'accuracy': epoch_acc,\n",
    "                'val_accuracy': val_acc\n",
    "            })     \n",
    "        liveloss.draw()\n",
    "        \n",
    "        tt = perf_counter()\n",
    "        elapsed = tt - ts\n",
    "        \n",
    "        print('Train Loss: {:.5f} Acc: {:.5f}'.format(epoch_loss, epoch_acc))\n",
    "        print(  'Val Loss: {:.5f} Acc: {:.5f}'.format(val_loss, val_acc))\n",
    "        print('Epoch Time: {:.4f}'.format(elapsed))\n",
    "        print('______________________________')\n",
    "    \n",
    "    # save the model if required\n",
    "    if save:\n",
    "        print(\"Saving the best trained model.\")\n",
    "        util.save_models(best_model_wts, save_folder, suffix=_ARCH+_SUFFIX+today)\n",
    "\n",
    "    time_elapsed = perf_counter() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Validation Accuracy: {}, Epoch: {}'.format(best_acc, best))\n",
    "    \n",
    "    return loss_list, accuracy_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accu(testloader, net, device):\n",
    "    net.to(device)\n",
    "    cnt_out = np.zeros(9) # this 9 is hardcoded for ResNet-20\n",
    "    cnt_high = np.zeros(9) # this 9 is hardcoded for ResNet-20\n",
    "    num_out = []\n",
    "    num_high = []\n",
    "    def _report_sparsity(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if isinstance(m, q.PGConv2d):\n",
    "            num_out.append(m.num_out)\n",
    "            num_high.append(m.num_high)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    # switch the model to the evaluation mode\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            \n",
    "            criterion = (nn.CrossEntropyLoss().cuda() \n",
    "                if torch.cuda.is_available() else nn.CrossEntropyLoss())\n",
    "            loss = criterion(outputs, labels)\n",
    "            for name, param in net.named_parameters():\n",
    "                if 'threshold' in name:\n",
    "                    loss += sigma * torch.norm(param-gtarget)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            \"\"\" calculate statistics per PG layer \"\"\"\n",
    "            if pg:\n",
    "                net.apply(_report_sparsity)\n",
    "                cnt_out += np.array(num_out)\n",
    "                cnt_high += np.array(num_high)\n",
    "                num_out = []\n",
    "                num_high = []\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %.1f %%' % (\n",
    "        100 * correct / total))\n",
    "    if pg:\n",
    "        print('Sparsity of the update phase: %.1f %%' % (100-np.sum(cnt_high)*1.0/np.sum(cnt_out)*100))\n",
    "    \n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_test_accu(testloader, classes, net, device):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %.1f %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 20-06-24\n",
      "Save at C:\\Users\\Aperture\\Git\\dnn-gating\\save_CIFAR10_model\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today().strftime(\"%y-%m-%d\")\n",
    "print(\"Today is\", today)\n",
    "\n",
    "batch_size = 128\n",
    "num_epoch = 200\n",
    "_LAST_EPOCH = -1 #last_epoch arg is useful for restart\n",
    "_WEIGHT_DECAY = 1e-4\n",
    "_ARCH = \"ternResNet\"\n",
    "this_file_path = os.path.abspath('./')\n",
    "save_folder = os.path.join(this_file_path, 'save_CIFAR10_model')\n",
    "print('Save at', save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = None\n",
    "save = True\n",
    "test = False\n",
    "wbits = 8\n",
    "abits = 5\n",
    "pact = True\n",
    "pbits = 3\n",
    "gtarget = 1\n",
    "sparse_bp = True\n",
    "pg = False\n",
    "sigma = 0.001\n",
    "\n",
    "_SUFFIX = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Create ternResNet model.\n",
      "CifarResNet : Depth : 20 , Layers for each block : 3\n",
      "CifarResNet(\n",
      "  (conv_1_3x3): quanConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (stage_1): Sequential(\n",
      "    (0): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (stage_2): Sequential(\n",
      "    (0): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): DownsampleA(\n",
      "        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (stage_3): Sequential(\n",
      "    (0): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): DownsampleA(\n",
      "        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResNetBasicblock(\n",
      "      (conv_a): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv_b): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (classifier): quanLinear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Available GPUs: {}\".format(torch.cuda.device_count()))\n",
    "\n",
    "print(\"Create {} model.\".format(_ARCH))\n",
    "net = generate_model()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Start training.\n",
      "current learning rate = 0.0001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Legacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3b551f933a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mloss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_accu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#     per_class_test_accu(testloader, classes, net, device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-604a25b4a203>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(trainloader, dataset_sizes, testloader, net, device)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\dnn-gating\\model\\tern_resnet_cifar.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_1_3x3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\dnn-gating\\model\\tern_resnet_cifar.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtfactor_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_quanFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfactor_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         raise RuntimeError(\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[1;34m\"Legacy autograd function with non-static forward method is deprecated. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;34m\"Please use new-style autograd function with static forward method. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \"(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\")\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Legacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if path:\n",
    "    print(\"@ Load trained model from {}.\".format(path))\n",
    "    net.load_state_dict(torch.load(path))\n",
    "\n",
    "print(\"Loading the data.\")\n",
    "trainloader, testloader, classes, dataset_sizes = load_cifar10()\n",
    "if test:\n",
    "    print(\"Mode: Test only.\")\n",
    "    val_loss, val_acc = test_accu(testloader, net, device)\n",
    "else:\n",
    "    print(\"Start training.\")\n",
    "    loss_list, accuracy_list, val_loss_list, val_acc_list = train_model(trainloader, dataset_sizes, testloader, net, device)\n",
    "    val_loss, val_acc = test_accu(testloader, net, device)\n",
    "#     per_class_test_accu(testloader, classes, net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename_los = \"./training_lists/\" + _ARCH + \"los\" + _SUFFIX + today + '.data'\n",
    "filename_acc = \"./training_lists/\" + _ARCH + \"acc\" + _SUFFIX + today + '.data'\n",
    "filename_vallos = \"./training_lists/\" + _ARCH + \"vallos\" + _SUFFIX + today + '.data'\n",
    "filename_valacc = \"./training_lists/\" + _ARCH + \"valacc\" + _SUFFIX + today + '.data'\n",
    "\n",
    "with open(filename_los, 'wb') as filehandle:\n",
    "    pickle.dump(loss_list, filehandle)\n",
    "with open(filename_acc, 'wb') as filehandle:\n",
    "    pickle.dump(accuracy_list, filehandle)\n",
    "with open(filename_vallos, 'wb') as filehandle:\n",
    "    pickle.dump(val_loss_list, filehandle)\n",
    "with open(filename_valacc, 'wb') as filehandle:\n",
    "    pickle.dump(val_acc_list, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), dpi=144, constrained_layout=True)\n",
    "gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "xlim = len(loss_list) + 1\n",
    "x = np.arange(1, xlim)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.set_xlim(-1, xlim)\n",
    "ax1.set_ylim(0, 1.02)\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.plot(x, accuracy_list, linestyle='-', label='training')\n",
    "ax1.plot(x, val_acc_list, linestyle='-', label='validation')\n",
    "plt.yticks(np.arange(0, 1.02, 0.1))\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.set_xlim(-1, xlim)\n",
    "ax2.set_ylim(0, 2.04)\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.plot(x, loss_list, linestyle='-', label='training')\n",
    "ax2.plot(x, val_loss_list, linestyle='-', label='validation')\n",
    "plt.yticks(np.arange(0, 2.04, 0.2))\n",
    "plt.grid(True)\n",
    "plt.title('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
