{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.utils as util\n",
    "\n",
    "import numpy as np\n",
    "import os, time, sys\n",
    "import argparse\n",
    "\n",
    "import utils.pg_utils as q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform_train = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomCrop(32, 4),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "        ])\n",
    "\n",
    "    # pin_memory=True makes transfering data from host to GPU faster\n",
    "    trainset = torchvision.datasets.CIFAR10(root='/tmp/cifar10_data', train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='/tmp/cifar10_data', train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    dataset_sizes = [len(trainset), len(testset)]\n",
    "    \n",
    "    return trainloader, testloader, classes, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.vgg\n",
    "def generate_model():\n",
    "    num_classes = 200\n",
    "    model_ft = model.vgg.vgg16_bn(pretrained = False, progress = True, num_classes = num_classes)\n",
    "    avg_pool = 2\n",
    "    model_ft.avgpool = nn.AdaptiveAvgPool2d((avg_pool, avg_pool))\n",
    "    model_ft.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * avg_pool * avg_pool, 4096),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    return model_ft\n",
    "#     if model_arch == 'resnet-20':\n",
    "#         if pg:\n",
    "#             import model.pg_cifar10_resnet_s as m\n",
    "#             kwargs = {'wbits':wbits, 'abits':abits, \\\n",
    "#                       'pred_bits':pbits, 'sparse_bp':sparse_bp, \\\n",
    "#                       'pact':pact}\n",
    "#             return m.resnet20(**kwargs)\n",
    "#         else:\n",
    "#             import model.quantized_cifar10_resnet as m\n",
    "#             kwargs = {'wbits':wbits, 'abits':abits, 'pact':pact}\n",
    "#             return m.resnet20(**kwargs)\n",
    "#     else:\n",
    "#         raise NotImplementedError(\"Model architecture is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "import copy\n",
    "\n",
    "def train_model(trainloader, dataset_sizes, testloader, net, device):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        print(\"Activate multi GPU support.\")\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    # define the loss function\n",
    "    criterion = (nn.CrossEntropyLoss().cuda() \n",
    "                if torch.cuda.is_available() else nn.CrossEntropyLoss())\n",
    "    # Scale the lr linearly with the batch size. \n",
    "    # Should be 0.1 when batch_size=128\n",
    "    initial_lr = 0.025\n",
    "    # initialize the optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), \n",
    "                          lr=initial_lr, \n",
    "                          momentum=0.9,\n",
    "                          weight_decay=1e-4)\n",
    "    # multiply the lr by 0.1 at 100, 150, and 200 epochs\n",
    "    div = num_epoch // 4\n",
    "    lr_decay_milestones = [200, 250, 300, 350]\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                        optimizer, \n",
    "                        milestones=lr_decay_milestones, \n",
    "                        gamma=0.1,\n",
    "                        last_epoch=-1)\n",
    "    \n",
    "    # some bookkeeping\n",
    "    since = perf_counter()\n",
    "    liveloss = PlotLosses()\n",
    "    best_acc = 0.0\n",
    "    best = 0\n",
    "    \n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    max_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epoch): # loop over the dataset multiple times\n",
    "        ts = perf_counter()\n",
    "        # set printing functions\n",
    "        batch_time = util.AverageMeter('Time/batch', ':.3f')\n",
    "        losses = util.AverageMeter('Loss', ':6.2f')\n",
    "        top1 = util.AverageMeter('Acc', ':6.2f')\n",
    "#         progress = util.ProgressMeter(\n",
    "#                         len(trainloader),\n",
    "#                         [losses, top1, batch_time],\n",
    "#                         prefix=\"Epoch: [{}]\".format(epoch+1)\n",
    "#                         )\n",
    "\n",
    "        # switch the model to the training mode\n",
    "        net.train()\n",
    "\n",
    "        print('current learning rate = {}'.format(optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        # each epoch\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        end = perf_counter()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            for name, param in net.named_parameters():\n",
    "                if 'threshold' in name:\n",
    "                    loss += sigma * torch.norm(param-gtarget)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            _, batch_predicted = torch.max(outputs.data, 1)\n",
    "            batch_accu = 100.0 * (batch_predicted == labels).sum().item() / labels.size(0)\n",
    "            batch_loss = loss.item() * inputs.size(0)\n",
    "            losses.update(loss.item(), labels.size(0))\n",
    "            top1.update(batch_accu, labels.size(0))\n",
    "            running_loss += batch_loss\n",
    "            running_corrects += torch.sum(batch_predicted == labels.data)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_elapsed = perf_counter() - end\n",
    "            batch_time.update(batch_elapsed)\n",
    "            end = perf_counter()\n",
    "\n",
    "#             progress.display(i) # i = batch id in the epoch\n",
    "            \n",
    "            print(\"\\r[Epoch:%d Iteration:%d/%d] Loss: %.3f, Accuracy: %.3f, Time: %.3f\"\n",
    "                  %(epoch+1, i+1, len(trainloader), batch_loss, batch_accu, batch_elapsed), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_sizes[0]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[0]\n",
    "        loss_list.append(epoch_loss)\n",
    "        accuracy_list.append(epoch_acc)\n",
    "        \n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # print test accuracy every epochs\n",
    "#         print('epoch {}'.format(epoch+1))\n",
    "        val_loss, val_acc = test_accu(testloader, net, device)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "\n",
    "        liveloss.update({\n",
    "                'log loss': epoch_loss,\n",
    "                'val_log loss': val_loss,\n",
    "                'accuracy': epoch_acc,\n",
    "                'val_accuracy': val_acc\n",
    "            })     \n",
    "        liveloss.draw()\n",
    "        \n",
    "        tt = perf_counter()\n",
    "        elapsed = tt - ts\n",
    "        \n",
    "        print('Train Loss: {:.5f} Acc: {:.5f}'.format(epoch_loss, epoch_acc))\n",
    "        print(  'Val Loss: {:.5f} Acc: {:.5f}'.format(val_loss, val_acc))\n",
    "        print('Epoch Time: {:.4f}'.format(elapsed))\n",
    "        print('______________________________')\n",
    "    \n",
    "    # save the model if required\n",
    "    if save:\n",
    "        print(\"Saving the best trained model.\")\n",
    "        util.save_models(best_model_wts, save_folder, suffix=_ARCH+_SUFFIX+today)\n",
    "\n",
    "    time_elapsed = perf_counter() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Validation Accuracy: {}, Epoch: {}'.format(best_acc, best))\n",
    "    \n",
    "    return loss_list, accuracy_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accu(testloader, net, device):\n",
    "    net.to(device)\n",
    "    cnt_out = np.zeros(9) # this 9 is hardcoded for ResNet-20\n",
    "    cnt_high = np.zeros(9) # this 9 is hardcoded for ResNet-20\n",
    "    num_out = []\n",
    "    num_high = []\n",
    "    def _report_sparsity(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if isinstance(m, q.PGConv2d):\n",
    "            num_out.append(m.num_out)\n",
    "            num_high.append(m.num_high)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    # switch the model to the evaluation mode\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            \n",
    "            criterion = (nn.CrossEntropyLoss().cuda() \n",
    "                if torch.cuda.is_available() else nn.CrossEntropyLoss())\n",
    "            loss = criterion(outputs, labels)\n",
    "            for name, param in net.named_parameters():\n",
    "                if 'threshold' in name:\n",
    "                    loss += sigma * torch.norm(param-gtarget)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            \"\"\" calculate statistics per PG layer \"\"\"\n",
    "            if pg:\n",
    "                net.apply(_report_sparsity)\n",
    "                cnt_out += np.array(num_out)\n",
    "                cnt_high += np.array(num_high)\n",
    "                num_out = []\n",
    "                num_high = []\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %.1f %%' % (\n",
    "        100 * correct / total))\n",
    "    if pg:\n",
    "        print('Sparsity of the update phase: %.1f %%' % (100-np.sum(cnt_high)*1.0/np.sum(cnt_out)*100))\n",
    "    \n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_test_accu(testloader, classes, net, device):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %.1f %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 20-06-24\n",
      "Save at C:\\Users\\Aperture\\Git\\dnn-gating\\save_CIFAR10_model\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today().strftime(\"%y-%m-%d\")\n",
    "print(\"Today is\", today)\n",
    "\n",
    "batch_size = 128\n",
    "num_epoch = 400\n",
    "_LAST_EPOCH = -1 #last_epoch arg is useful for restart\n",
    "_WEIGHT_DECAY = 1e-4\n",
    "_ARCH = \"VGG16bn\"\n",
    "this_file_path = os.path.abspath('./')\n",
    "save_folder = os.path.join(this_file_path, 'save_CIFAR10_model')\n",
    "print('Save at', save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = None\n",
    "save = True\n",
    "test = False\n",
    "wbits = 8\n",
    "abits = 5\n",
    "pact = True\n",
    "pbits = 3\n",
    "gtarget = 1\n",
    "sparse_bp = True\n",
    "pg = False\n",
    "sigma = 0.001\n",
    "\n",
    "_SUFFIX = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Create VGG16bn model.\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): quanConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): quanConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): quanConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (25): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(2, 2))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (1): Linear(in_features=4096, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Available GPUs: {}\".format(torch.cuda.device_count()))\n",
    "\n",
    "print(\"Create {} model.\".format(_ARCH))\n",
    "net = generate_model()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Start training.\n",
      "current learning rate = 0.025\n",
      "[Epoch:1 Iteration:391/391] Loss: 177.339, Accuracy: 16.250, Time: 0.165"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8ac8544549e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mloss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_accu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#     per_class_test_accu(testloader, classes, net, device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d761218ca1f8>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(trainloader, dataset_sizes, testloader, net, device)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# print test accuracy every epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;31m#         print('epoch {}'.format(epoch+1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_accu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mval_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mval_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-eddb097da0be>\u001b[0m in \u001b[0;36mtest_accu\u001b[1;34m(testloader, net, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             criterion = (nn.CrossEntropyLoss().cuda() \n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\dnn-gating\\model\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\dnn-gating\\model\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtfactor_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# input = _quanFunc().apply(input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_quanFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\dnn-gating\\model\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, input)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if path:\n",
    "    print(\"@ Load trained model from {}.\".format(path))\n",
    "    net.load_state_dict(torch.load(path))\n",
    "\n",
    "print(\"Loading the data.\")\n",
    "trainloader, testloader, classes, dataset_sizes = load_cifar10()\n",
    "if test:\n",
    "    print(\"Mode: Test only.\")\n",
    "    val_loss, val_acc = test_accu(testloader, net, device)\n",
    "else:\n",
    "    print(\"Start training.\")\n",
    "    loss_list, accuracy_list, val_loss_list, val_acc_list = train_model(trainloader, dataset_sizes, testloader, net, device)\n",
    "    val_loss, val_acc = test_accu(testloader, net, device)\n",
    "#     per_class_test_accu(testloader, classes, net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename_los = \"./training_lists/\" + _ARCH + \"los\" + _SUFFIX + today + '.data'\n",
    "filename_acc = \"./training_lists/\" + _ARCH + \"acc\" + _SUFFIX + today + '.data'\n",
    "filename_vallos = \"./training_lists/\" + _ARCH + \"vallos\" + _SUFFIX + today + '.data'\n",
    "filename_valacc = \"./training_lists/\" + _ARCH + \"valacc\" + _SUFFIX + today + '.data'\n",
    "\n",
    "with open(filename_los, 'wb') as filehandle:\n",
    "    pickle.dump(loss_list, filehandle)\n",
    "with open(filename_acc, 'wb') as filehandle:\n",
    "    pickle.dump(accuracy_list, filehandle)\n",
    "with open(filename_vallos, 'wb') as filehandle:\n",
    "    pickle.dump(val_loss_list, filehandle)\n",
    "with open(filename_valacc, 'wb') as filehandle:\n",
    "    pickle.dump(val_acc_list, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), dpi=144, constrained_layout=True)\n",
    "gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "xlim = len(loss_list) + 1\n",
    "x = np.arange(1, xlim)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.set_xlim(-1, xlim)\n",
    "ax1.set_ylim(0, 1.02)\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.plot(x, accuracy_list, linestyle='-', label='training')\n",
    "ax1.plot(x, val_acc_list, linestyle='-', label='validation')\n",
    "plt.yticks(np.arange(0, 1.02, 0.1))\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.set_xlim(-1, xlim)\n",
    "ax2.set_ylim(0, 2.04)\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.plot(x, loss_list, linestyle='-', label='training')\n",
    "ax2.plot(x, val_loss_list, linestyle='-', label='validation')\n",
    "plt.yticks(np.arange(0, 2.04, 0.2))\n",
    "plt.grid(True)\n",
    "plt.title('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
